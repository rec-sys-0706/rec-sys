{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from flask import Flask\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "params=urllib.parse.quote_plus('Driver={ODBC Driver 17 for SQL Server};Server=LAPTOP-LDP5RKBS;Database=News;Trusted_Connection=yes;')\n",
    "\n",
    "app=Flask(__name__)\n",
    "app.config['SQLALCHEMY_DATABASE_URI']=f'mssql+pyodbc:///?odbc_connect={params}'\n",
    "db=SQLAlchemy(app)\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__='stories'\n",
    "    id=db.Column(db.INTEGER,primary_key=True)\n",
    "    category = db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    subcategory = db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    title=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    date=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    abstract=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    contents=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    url=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self,category,subcategory,title,date,abstract,contents,url):\n",
    "        self.category=category\n",
    "        self.subcategory=subcategory\n",
    "        self.title=title\n",
    "        self.date=date\n",
    "        self.abstract=abstract\n",
    "        self.contents=contents\n",
    "        self.url=url\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__='videos'\n",
    "    id=db.Column(db.INTEGER,primary_key=True)\n",
    "    title=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    date=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    abstract=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    contents=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    url=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    \n",
    "    \n",
    "    def __init__(self,title,date,abstract,contents,url):\n",
    "        self.title=title\n",
    "        self.date=date\n",
    "        self.abstract=abstract\n",
    "        self.contents=contents\n",
    "        self.url=url\n",
    "\n",
    "if __name__=='__main__':\n",
    "    with app.app_context():\n",
    "        db.drop_all()\n",
    "        db.create_all()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from flask import Flask\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "# 初始化 Flask 應用程式與資料庫\n",
    "def create_app(SQLSERVER, SQL_DATABASE, SQL_USERNAME,SQL_PASSWORD):\n",
    "    app = Flask(__name__)\n",
    "    params = urllib.parse.quote_plus(\n",
    "        f'Driver={{ODBC Driver 17 for SQL Server}};Server={SQLSERVER};Database={SQL_DATABASE};UID={SQL_USERNAME};PWD={SQL_PASSWORD};Trusted_Connection=yes;'\n",
    "    )\n",
    "    app.config['SQLALCHEMY_DATABASE_URI'] = f'mssql+pyodbc:///?odbc_connect={params}'\n",
    "    return app\n",
    "\n",
    "# 初始化資料庫\n",
    "def init_db(app):\n",
    "    db = SQLAlchemy(app)\n",
    "    return db\n",
    "\n",
    "# 定義 Stories Model\n",
    "def define_stories_model(db):\n",
    "    class Stories(db.Model):\n",
    "        __tablename__ = 'stories'\n",
    "        id = db.Column(db.INTEGER, primary_key=True)\n",
    "        category = db.Column(db.String('max'), unique=False, nullable=False)\n",
    "        subcategory = db.Column(db.String('max'), unique=False, nullable=False)\n",
    "        title = db.Column(db.String('max'), unique=False, nullable=False)\n",
    "        date = db.Column(db.String('max'), unique=False, nullable=False)\n",
    "        abstract = db.Column(db.String('max'), unique=False, nullable=False)\n",
    "        contents = db.Column(db.String('max'), unique=False, nullable=False)\n",
    "        url = db.Column(db.String('max'), unique=False, nullable=False)\n",
    "\n",
    "        def __init__(self, category, subcategory, title, date, abstract, contents, url):\n",
    "            self.category = category\n",
    "            self.subcategory = subcategory\n",
    "            self.title = title\n",
    "            self.date = date\n",
    "            self.abstract = abstract\n",
    "            self.contents = contents\n",
    "            self.url = url\n",
    "\n",
    "    return Stories\n",
    "\n",
    "# 建立資料庫\n",
    "def setup_database(app, db):\n",
    "    with app.app_context():\n",
    "        db.drop_all()\n",
    "        db.create_all()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = create_app(\"SQLSERVER\", \"SQL_DATABASE\", \"SQL_USERNAME\",\"SQL_PASSWORD\")\n",
    "    db = init_db(app)\n",
    "    \n",
    "    Stories = define_stories_model(db)\n",
    "    \n",
    "    setup_database(app, db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章\n",
    "import pyodbc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "def setup_database_connection():\n",
    "    conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\n",
    "                          'Server=SQLSERVER;'\n",
    "                          'Database=SQL_DATABASE;'\n",
    "                          'UID=SQL_USERNAME;'\n",
    "                          'PWD=SQL_PASSWORD;'\n",
    "                          'Trusted_Connection=yes;')\n",
    "    return conn\n",
    "\n",
    "def setup_webdriver(chrome_path):\n",
    "    service = Service(chrome_path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.implicitly_wait(10)\n",
    "    return driver\n",
    "\n",
    "def perform_search(driver, url, keyword):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.ID, 'headerSearchIcon').click()\n",
    "    driver.find_element(By.CLASS_NAME, 'search-bar__input').send_keys(keyword)\n",
    "    driver.find_element(By.CLASS_NAME, 'search-bar__submit').click()\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[1]/div[2]/div/div/ul/li[2]/label').click()\n",
    "    time.sleep(10)\n",
    "\n",
    "def extract_articles(driver, cur, page_count=1):\n",
    "    for _ in range(page_count):\n",
    "        content_div = driver.find_element(By.CLASS_NAME, 'container__field-links')\n",
    "        divs_in_content = content_div.find_elements(By.XPATH, './div')\n",
    "        div_count = len(divs_in_content)\n",
    "        \n",
    "        for x in range(1, div_count + 1):\n",
    "            extract_and_save_article(driver, cur, x)\n",
    "        navigate_to_next_page(driver)\n",
    "\n",
    "def extract_and_save_article(driver, cur, index):\n",
    "    titles = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{index}]/a[2]/div/div[1]/span').text\n",
    "    publication_dates = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{index}]/a[2]/div/div[2]').text\n",
    "    abstracts = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{index}]/a[2]/div/div[3]').text\n",
    "    elements = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{index}]/a[2]')\n",
    "    urls = elements.get_attribute('href')\n",
    "\n",
    "    driver.get(urls)\n",
    "    categories, subcategories = extract_categories(driver)\n",
    "    content = extract_content(driver)\n",
    "\n",
    "    cur.execute('''INSERT INTO stories(category,subcategory,title,date,abstract,contents,url) \n",
    "                   VALUES(?,?,?,?,?,?,?);''', (categories, subcategories, titles, publication_dates, abstracts, content, urls))\n",
    "    cur.connection.commit()\n",
    "    driver.back()\n",
    "    time.sleep(1)\n",
    "\n",
    "def extract_categories(driver):\n",
    "    try:\n",
    "        categories = driver.find_element(By.CLASS_NAME, 'breadcrumb__parent-link').text\n",
    "        subcategories = driver.find_element(By.CLASS_NAME, 'breadcrumb__child-link').text\n",
    "    except:\n",
    "        categories = \" \"\n",
    "        subcategories = \" \"\n",
    "    return categories, subcategories\n",
    "\n",
    "def extract_content(driver):\n",
    "    try:\n",
    "        article_content = driver.find_element(By.CLASS_NAME, 'article__content')\n",
    "        time.sleep(2)\n",
    "        paragraphs = article_content.find_elements(By.TAG_NAME, 'p')\n",
    "        time.sleep(2)\n",
    "        content = \"\\n\".join([p.text for p in paragraphs])\n",
    "    except:\n",
    "        content = \" \"\n",
    "    return content\n",
    "\n",
    "def navigate_to_next_page(driver):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[4]/div/div[3]').click()\n",
    "        time.sleep(10)\n",
    "    except:\n",
    "        print(\"No more pages.\")\n",
    "\n",
    "def main():\n",
    "    conn = setup_database_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.fast_executemany = True\n",
    "\n",
    "    chrome_path = \"../chromedriver-win32/chromedriver.exe\"\n",
    "\n",
    "    driver = setup_webdriver(chrome_path)\n",
    "\n",
    "    url = 'https://edition.cnn.com/'\n",
    "    keyword = 'Artificial Intelligence'\n",
    "    \n",
    "    perform_search(driver, url, keyword)\n",
    "    extract_articles(driver, cur, page_count=4)  # 控制頁數\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章類\n",
    "import pyodbc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "conn=pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};''Server=LAPTOP-LDP5RKBS;''Database=News;''Trusted_Connection=yes;')\n",
    "chrome_path = 'C:\\\\Users\\\\USER\\\\Downloads\\\\chromedriver-win32\\\\chromedriver-win32\\\\chromedriver.exe'  \n",
    "service = Service(chrome_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = 'https://edition.cnn.com/'\n",
    "driver.get(url)\n",
    "cur = conn.cursor()\n",
    "cur.fast_executemany = True\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element(By.ID, 'headerSearchIcon').click() \n",
    "\n",
    "keyword = 'Artificial Intelligence'\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__input').send_keys(keyword)\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__submit').click() \n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[1]/div[2]/div/div/ul/li[2]/label').click() \n",
    "time.sleep(10)\n",
    "\n",
    "content_div = driver.find_element(By.CLASS_NAME, 'container__field-links')\n",
    "divs_in_content = content_div.find_elements(By.XPATH, './div')\n",
    "div_count = len(divs_in_content)\n",
    "\n",
    "#第一頁\n",
    "for x in range(1, div_count+1):                 \n",
    "    titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[1]/span').text  #標題\n",
    "    publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[2]').text   #日期  \n",
    "    abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "    elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]')\n",
    "    urls = elements.get_attribute('href')   #連結\n",
    "    \n",
    "    driver.get(urls)    \n",
    "    try:\n",
    "        categories = driver.find_element(By.CLASS_NAME, 'breadcrumb__parent-link').text  #類別\n",
    "        subcategories = driver.find_element(By.CLASS_NAME, 'breadcrumb__child-link').text  #子類別\n",
    "    except:\n",
    "        categories = \" \"\n",
    "        subcategories = \" \"\n",
    "        \n",
    "    try:\n",
    "        a = driver.find_element(By.CLASS_NAME, 'article__content')\n",
    "        time.sleep(2)\n",
    "        paragraphs = a.find_elements(By.TAG_NAME, 'p')\n",
    "        time.sleep(2)\n",
    "        content_str = [p.text for p in paragraphs]\n",
    "        content = \"\\n\".join(content_str)\n",
    "    except:\n",
    "        content = \" \"\n",
    "        \n",
    "    cur.execute('''INSERT INTO stories(category,subcategory,title,date,abstract,contents,url) VALUES(?,?,?,?,?,?,?);''',(categories,subcategories,titles,publication_dates,abstracts,content,urls))\n",
    "    conn.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.back()\n",
    "    time.sleep(1)\n",
    "\n",
    "for y in range(1,4):               \n",
    "    pages=driver.find_element(By.XPATH,'//*[@id=\"search\"]/div[2]/div/div[4]/div/div[3]').click()\n",
    "    time.sleep(10) \n",
    "    content_div_ano = driver.find_element(By.CLASS_NAME, 'container__field-links')\n",
    "    divs_in_content_ano = content_div_ano.find_elements(By.XPATH, './div')\n",
    "    div_count_ano = len(divs_in_content_ano)\n",
    "    for z in range(1, div_count_ano+1):                     \n",
    "        titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[1]/span').text  #標題\n",
    "        publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[2]').text   #日期  \n",
    "        abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "        elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]')\n",
    "        urls = elements.get_attribute('href')   #連結\n",
    "        \n",
    "        driver.get(urls)    \n",
    "        try:\n",
    "            categories = driver.find_element(By.CLASS_NAME, 'breadcrumb__parent-link').text  #類別\n",
    "            subcategories = driver.find_element(By.CLASS_NAME, 'breadcrumb__child-link').text  #子類別\n",
    "        except:\n",
    "            categories = \" \"\n",
    "            subcategories = \" \"\n",
    "        \n",
    "        try:\n",
    "            a = driver.find_element(By.CLASS_NAME, 'article__content')\n",
    "            time.sleep(2)\n",
    "            paragraphs = a.find_elements(By.TAG_NAME, 'p')\n",
    "            time.sleep(2)\n",
    "            content_str = [p.text for p in paragraphs]\n",
    "            content = \"\\n\".join(content_str)\n",
    "        except:\n",
    "            content = \" \"\n",
    "\n",
    "        cur.execute('''INSERT INTO stories(category,subcategory,title,date,abstract,contents,url) VALUES(?,?,?,?,?,?,?);''',(categories,subcategories,titles,publication_dates,abstracts,content,urls))\n",
    "        conn.commit()\n",
    "        time.sleep(1)\n",
    "\n",
    "        driver.back()\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.quit() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#影片類\n",
    "import pyodbc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "conn=pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};''Server=DESKTOP-4JUNPCV;''Database=News;''Trusted_Connection=yes;')\n",
    "chrome_path = 'C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe'  \n",
    "service = Service(chrome_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = 'https://edition.cnn.com/'\n",
    "driver.get(url)\n",
    "cur = conn.cursor()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element(By.ID, 'headerSearchIcon').click() \n",
    "\n",
    "keyword = 'Artificial Intelligence'\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__input').send_keys(keyword)\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__submit').click() \n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[1]/div[2]/div/div/ul/li[3]/label').click() \n",
    "time.sleep(10)\n",
    "\n",
    "#第一頁\n",
    "for x in range(1,7):                 \n",
    "    titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[1]/span').text  #標題\n",
    "    publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[2]').text   #日期  \n",
    "    abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "    elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]')\n",
    "    urls = elements.get_attribute('href')   #連結\n",
    "    \n",
    "    driver.get(urls)           \n",
    "    a = driver.find_elements(By.XPATH, '/html/body/div[1]/section[4]/section/div/section/div/div/div[1]/div/div[3]/div[5]')\n",
    "    time.sleep(2)\n",
    "    for item in a:\n",
    "        content = item.text   #內文\n",
    "    cur.execute('''INSERT INTO videos(title,date,abstract,contents,url) VALUES(?,?,?,?,?);''',(titles,publication_dates,abstracts,content,urls))\n",
    "    conn.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.back()\n",
    "    time.sleep(1)\n",
    "    \n",
    "for y in range(1,9):               \n",
    "    pages=driver.find_element(By.XPATH,'//*[@id=\"search\"]/div[2]/div/div[4]/div/div[3]').click()\n",
    "    time.sleep(10) \n",
    "    for z in range(1,11):                     \n",
    "        titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[1]/span').text  #標題\n",
    "        publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[2]').text   #日期  \n",
    "        abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "        elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]')\n",
    "        urls = elements.get_attribute('href')   #連結\n",
    "\n",
    "        driver.get(urls)           \n",
    "        a = driver.find_elements(By.XPATH, '/html/body/div[1]/section[4]/section/div/section/div/div/div[1]/div/div[3]/div[5]')\n",
    "        time.sleep(2)\n",
    "        for item in a:\n",
    "            content = item.text   #內文  \n",
    "        cur.execute('''INSERT INTO videos(title,date,abstract,contents,url) VALUES(?,?,?,?,?);''',(titles,publication_dates,abstracts,content,urls))\n",
    "        conn.commit()\n",
    "        time.sleep(1)\n",
    "\n",
    "        driver.back()\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
